{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dc47491d",
   "metadata": {},
   "source": [
    "# Exploring Tensors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8adee1e6",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "3f82f28d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b1c7a11",
   "metadata": {},
   "source": [
    "## Tensor Initialization  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "6605e3d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Tensor: \n",
      "tensor([[1, 2],\n",
      "        [3, 4]])\n",
      "\n",
      "NumPy Tensor: \n",
      "tensor([[1, 2],\n",
      "        [3, 4]], dtype=torch.int32)\n",
      "\n",
      "Ones Tensor: \n",
      "tensor([[1, 1],\n",
      "        [1, 1]])\n",
      "\n",
      "Random Tensor: \n",
      "tensor([[0.3973, 0.6803],\n",
      "        [0.3269, 0.3399]])\n",
      "\n",
      "Random Tensor: \n",
      "tensor([[0.3869, 0.0111, 0.0434],\n",
      "        [0.7623, 0.7386, 0.4639]])\n",
      "\n",
      "Random Tensor Type: torch.float32\n",
      "\n",
      "Ones Tensor: \n",
      "tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.]])\n",
      "\n",
      "Ones Tensor Type: torch.float32\n",
      "\n",
      "Zeros Tensor: \n",
      "tensor([[0., 0., 0.],\n",
      "        [0., 0., 0.]])\n",
      "\n",
      "Zeros Tensor Type: torch.float32\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Directly from data\n",
    "data = [[1, 2], [3, 4]]\n",
    "x_data = torch.tensor(data)\n",
    "print(f\"Data Tensor: \\n{x_data}\\n\")\n",
    "\n",
    "# From a NumPy array\n",
    "np_array = np.array(data) # data to NumPy array first\n",
    "np_tensor = torch.from_numpy(np_array)\n",
    "print(f\"NumPy Tensor: \\n{np_tensor}\\n\")\n",
    "\n",
    "# From another tensor\n",
    "x_ones = torch.ones_like(x_data)  # retains the properties (shape and datatype) of x_data unless specified otherwise\n",
    "print(f\"Ones Tensor: \\n{x_ones}\\n\")\n",
    "x_rand = torch.rand_like(x_data, dtype=torch.float)  # specificially overwrites the data type\n",
    "print(f\"Random Tensor: \\n{x_rand}\\n\")\n",
    "\n",
    "# With random or constant values\n",
    "# Shape is a tuple of tensor dimensions\n",
    "shape = (2, 3,)  # 2 rows, 3 columns\n",
    "rand_tensor = torch.rand(shape)\n",
    "ones_tensor = torch.ones(shape)\n",
    "zeros_tensor = torch.zeros(shape)\n",
    "\n",
    "print(f\"Random Tensor: \\n{rand_tensor}\\n\")\n",
    "print(f\"Random Tensor Type: {rand_tensor.dtype}\\n\")\n",
    "print(f\"Ones Tensor: \\n{ones_tensor}\\n\")\n",
    "print(f\"Ones Tensor Type: {ones_tensor.dtype}\\n\")\n",
    "print(f\"Zeros Tensor: \\n{zeros_tensor}\\n\")\n",
    "print(f\"Zeros Tensor Type: {zeros_tensor.dtype}\\n\")\n",
    "\n",
    "\n",
    "\n",
    "# comma at the end is important for single-element tuples: (2,) is not the same as (2)\n",
    "x = (3) # <class 'int'>\n",
    "x = (3,) # <class 'tuple'>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c23f9762",
   "metadata": {},
   "source": [
    "## Tensor Attributes  \n",
    "\n",
    "Shape descriptions, datatype, device stored on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "bfcb8629",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor Shape: torch.Size([3, 4])\n",
      "Tensor Data Type: torch.float32\n",
      "Tensor Device: cpu\n"
     ]
    }
   ],
   "source": [
    "tensor = torch.rand(3, 4)  # 3 rows, 4 columns\n",
    "\n",
    "print(f\"Tensor Shape: {tensor.shape}\")  # prints the shape of the tensor\n",
    "print(f\"Tensor Data Type: {tensor.dtype}\")  # prints the data type of the tensor\n",
    "print(f\"Tensor Device: {tensor.device}\")  # prints the device where the tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8da42eac",
   "metadata": {},
   "source": [
    "## Tensor Operations  \n",
    "\n",
    "Just some basic ones. There are over 100 available in PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "903b073e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.]])\n",
      "tensor([[1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1.]])\n",
      "tensor([[1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.]])\n",
      "tensor.mul(tensor) \n",
      " tensor([[1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.]]) \n",
      "\n",
      "tensor * tensor \n",
      " tensor([[1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.]])\n",
      "tensor.matmul(tensor.T) \n",
      " tensor([[3., 3., 3., 3.],\n",
      "        [3., 3., 3., 3.],\n",
      "        [3., 3., 3., 3.],\n",
      "        [3., 3., 3., 3.]]) \n",
      "\n",
      "tensor @ tensor.T \n",
      " tensor([[3., 3., 3., 3.],\n",
      "        [3., 3., 3., 3.],\n",
      "        [3., 3., 3., 3.],\n",
      "        [3., 3., 3., 3.]])\n",
      "tensor([[1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.]]) \n",
      "\n",
      "tensor([[6., 5., 6., 6.],\n",
      "        [6., 5., 6., 6.],\n",
      "        [6., 5., 6., 6.],\n",
      "        [6., 5., 6., 6.]])\n"
     ]
    }
   ],
   "source": [
    "# Move tensor to GPU if available\n",
    "if torch.cuda.is_available():\n",
    "    tensor = tensor.to('cuda')\n",
    "    print(f\"Tensor moved to Device: {tensor.device}\")  # prints the device where the tensor is now stored\n",
    "\n",
    "# Standard NumPy-like indexing and slicing    \n",
    "tensor = torch.ones(4, 4)\n",
    "tensor[:,1] = 0 # for all rows, second column (index 1) make 0\n",
    "print(tensor)\n",
    "\n",
    "# Joining tensors (can also use torch.stack)\n",
    "t1 = torch.cat([tensor, tensor, tensor], dim=1) # dim is like axis in NumPy so concat on columns (\"extend right\")\n",
    "print(t1)\n",
    "t2 = torch.cat([tensor, tensor, tensor], dim=0) # dim is like axis in NumPy so concat on rows (\"extend down\")\n",
    "print(t2)\n",
    "\n",
    "# Multiplying tensors\n",
    "# This computes the element-wise product\n",
    "print(f\"tensor.mul(tensor) \\n {tensor.mul(tensor)} \\n\")\n",
    "# Alternative syntax:\n",
    "print(f\"tensor * tensor \\n {tensor * tensor}\")\n",
    "\n",
    "# Matrix multiplication\n",
    "print(f\"tensor.matmul(tensor.T) \\n {tensor.matmul(tensor.T)} \\n\")\n",
    "# Alternative syntax:\n",
    "print(f\"tensor @ tensor.T \\n {tensor @ tensor.T}\")\n",
    "\n",
    "# In-place operations: Use _ suffix\n",
    "print(tensor, \"\\n\")\n",
    "tensor.add_(5) # add 5 to each element in tensor\n",
    "print(tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6abab4a5",
   "metadata": {},
   "source": [
    "## Bridge with NumPy  \n",
    "\n",
    "Tensors on CPU and NumPy arrays can share underlying memory locations, and *changing one will change the other*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "1315e9a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch t: tensor([1., 1., 1., 1., 1.])\n",
      "numpy n: [1. 1. 1. 1. 1.]\n",
      "new t: tensor([2., 2., 2., 2., 2.])\n",
      "new n: [2. 2. 2. 2. 2.]\n",
      "numpy n: [1. 1. 1. 1. 1.]\n",
      "torch t: tensor([1., 1., 1., 1., 1.], dtype=torch.float64)\n",
      "new t: tensor([2., 2., 2., 2., 2.], dtype=torch.float64)\n",
      "new n: [2. 2. 2. 2. 2.]\n"
     ]
    }
   ],
   "source": [
    "# Tensor to NumPy array\n",
    "t = torch.ones(5)\n",
    "print(f\"torch t: {t}\")\n",
    "n = t.numpy()\n",
    "print(f\"numpy n: {n}\")\n",
    "\n",
    "# A change in the tensor reflects in the NumPy array\n",
    "# if use without _ suffix for out-of-place, must assign to new tensor: new_t = t.add(1)\n",
    "t.add_(1) # Torch tensor is an instance of a class, so methods are tied with tensor object\n",
    "print(f\"new t: {t}\")\n",
    "print(f\"new n: {n}\")\n",
    "\n",
    "n = np.ones(5)\n",
    "print(f\"numpy n: {n}\")\n",
    "t = torch.from_numpy(n)\n",
    "print(f\"torch t: {t}\")\n",
    "np.add(n, 1, out=n) # NumPy uses a function-oriented approach instead (array, value, out)\n",
    "print(f\"new t: {t}\")\n",
    "print(f\"new n: {n}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef0e9a65",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ultralytics-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
