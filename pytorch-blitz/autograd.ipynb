{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e940f9bc",
   "metadata": {},
   "source": [
    "# torch.autograd for automatic differentiation (CPU implementation)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "499b8561",
   "metadata": {},
   "source": [
    "## General Usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "39d347c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision.models import resnet18, ResNet18_Weights # trained on ImageNet dataset (1000 classes of objects)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "56402cee",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = resnet18(weights=ResNet18_Weights.DEFAULT)\n",
    "data = torch.rand(1, 3, 64, 64) # dummy image of (batch_size, channels, height, width)\n",
    "labels = torch.rand(1, 1000) # 1000 classes of objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a8848574",
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = model(data) # forward pass: the computational graph is implicitly built here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "206513e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = (prediction - labels).sum() # calculate error/loss\n",
    "loss.backward() # error tensor backward pass \n",
    "# autograd calculates and stores the gradients for each model parameter in the parameter's grad attribute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a15dcf95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load SGD optimizer: Register all model parameters, define LR and momentum\n",
    "optim = torch.optim.SGD(model.parameters(), lr=1e-2, momentum=0.9)\n",
    "optim.step() # initiate GD: optimizer adjusts each parameter by its gradient stored in .grad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba7423ea",
   "metadata": {},
   "source": [
    "## Differentiation in Autograd\n",
    "\n",
    "In the directed acyclic graph (DAG) / computational graph, leaves are input tensors, roots are output tensors. \n",
    "\n",
    "By tracing graph from roots to leaves, the gradients can be automatically computed using chain rule."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e4db7b17",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "a = torch.tensor([2., 3.], requires_grad=True) # tell autograd that every operation on this tensor must be tracked. dot after number is to init datatype as float\n",
    "b = torch.tensor([6., 4.], requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "99c454fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q = 3 * a ** 3 - b ** 2 # create new tensor from a and b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d90207f2",
   "metadata": {},
   "source": [
    "a and b represents parameters of NN and Q the error  \n",
    "\n",
    "**need to explicitly pass a gradient argument in Q.backward() because it is a vector. gradient is a tensor of the same shape as Q, and it represents the gradient of Q w.r.t itself.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "72cea167",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([True, True])\n",
      "tensor([True, True])\n"
     ]
    }
   ],
   "source": [
    "external_grad = torch.tensor([1., 1.])\n",
    "Q.backward(gradient=external_grad)\n",
    "# after each backward call, autograd starts populating a new graph (recreated from scratch each time)\n",
    "\n",
    "# Check if collected gradients are correct\n",
    "print(9 * a ** 2 == a.grad)\n",
    "print(-2 * b == b.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2826137",
   "metadata": {},
   "source": [
    "## Exclusion from the DAG\n",
    "\n",
    "Parameters that don't compute gradients are usually called **frozen parameters** which helps to offload computational demand. This is often done when performing finetuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "859582d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Does `a` require gradients?: False\n",
      "Does `b` require gradients?: True\n"
     ]
    }
   ],
   "source": [
    "x = torch.rand(5, 5)\n",
    "y = torch.rand(5, 5)\n",
    "z = torch.rand((5, 5), requires_grad=True)\n",
    "\n",
    "a = x + y\n",
    "print(f\"Does `a` require gradients?: {a.requires_grad}\") # both don't require gradients by default so output don't require gradients\n",
    "b = x + z\n",
    "print(f\"Does `b` require gradients?: {b.requires_grad}\") # at least one requires gradients so output requires gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3e61638",
   "metadata": {},
   "source": [
    "### Fine-Tuning Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "374075f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ResNet(\n",
      "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
      "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (relu): ReLU(inplace=True)\n",
      "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "  (layer1): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer3): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer4): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "  (fc): Linear(in_features=512, out_features=1000, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "from torch import nn, optim\n",
    "\n",
    "model  = resnet18(weights=ResNet18_Weights.DEFAULT) # imagenett1k_v1 weights\n",
    "\n",
    "# Freeze all parameters in network\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "    \n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aaf3c36",
   "metadata": {},
   "source": [
    "**Bias set to False mainly because using BatchNorm, since bias purpose is to allow layer to shift output values up or down, and BatchNorm essentially does this.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "970f5c11",
   "metadata": {},
   "source": [
    "Fine-tune model on new dataset with 10 labels.\n",
    "\n",
    "In restnet, classifier layer is last linear layer model.fc. Simply replace it with new linear layer (unfrozen by default) that acts as our classifier.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "509996d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fc = nn.Linear(512, 10) # new linear layer with unfrozen parameters\n",
    "# for tensors, default is requires_grad=False!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "5a09b86d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimize only the classifier (last layer)\n",
    "optimizer = optim.SGD(model.parameters(), lr=1e-2, momentum=0.9)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ultralytics-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
